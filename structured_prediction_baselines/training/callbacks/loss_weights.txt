from typing import List, Tuple, Union, Dict, Any, Optional
import torch
from allennlp.common.lazy import Lazy
from allennlp.training.trainer import (
    TrainerCallback, # shouldn't it be #from allennlp.training.callbacks.callback import TrainerCallback 
    GradientDescentTrainer,
)
import warnings
import logging

logger = logging.getLogger(__name__)

# @TrainerCallback.register("lossweight-custom")
# class LossWeightOnEpoch(TrainerCallback):
#     """
#     A callback that you can pass to the `GradientDescentTrainer` to access the current epoch number
#     in your model during training. This callback sets `model.epoch`, which can be read inside of
#     `model.forward()`. We set `model.epoch = epoch + 1` which now denotes the number of
#     completed epochs at a given training state.
#     """
#     def __init__(
#         self,
#         serialization_dir: str,
#         loss_idx_list: Optional[list] = None,
#         initial_weight_list: None,
#     ) -> None:
#         super().__init__(
#             serialization_dir=serialization_dir,
#             tensorboard_writer=tensorboard_writer,
#         )
#         self.loss_idx_list = loss_idx_list
#         self.initial_weight_list = [0.0]* len(loss_idx_list)

#     def get_loss_weights(self, trainer: "GradientDescentTrainer"):
#         for idx in self.loss_idx_list:
#             self.initial_weight_list = trainer.model.sampler.loss_fn.loss_weights[loss_idx] 

#     def set_loss_weights(self, trainer: "GradientDescentTrainer"):
#         for idx in self.loss_idx_list:
#             trainer.model.sampler.loss_fn.loss_weights[loss_idx] 

#     def on_start(
#         self, trainer: "GradientDescentTrainer", is_primary: bool = True, **kwargs
#     ) -> None:
#         super().on_start(trainer, is_primary)
#         # trainer.model.epoch = 0  # type: ignore[assignment]
#         self.get_loss_weights(trainer)

#     def on_epoch(
#         self,
#         trainer: "GradientDescentTrainer",
#         metrics: Dict[str, Any],
#         epoch: int,
#         is_primary: bool = True,
#         **kwargs,
#     ) -> None:
#         """
#         Overriding on_epoch to control the weights.
#         """
#         super.on_epoch()
#         trainer.mode.epoch




# @TrainerCallback.register("tensorboard-custom")
# class CustomTensorBoardCallback(TensorBoardCallback):
#     def __init__(
#         self,
#         serialization_dir: str,
#         tensorboard_writer: Lazy[TensorBoardWriter] = Lazy(TensorBoardWriter),
#         model_outputs_to_log: List[str] = None,
#     ) -> None:
#         super().__init__(
#             serialization_dir=serialization_dir,
#             tensorboard_writer=tensorboard_writer,
#         )
#         self._model_outputs_to_log = model_outputs_to_log or []
#         self._warned_about_missing_keys = False

#     def _warn_about_missing_keys(
#         self, model_outputs: List[Dict[str, Any]]
#     ) -> None:

#         if not self._warned_about_missing_keys:
#             for key in self._model_outputs_to_log:
#                 if key not in model_outputs[0]:
#                     logger.warning(f"Key {key} missing in model outputs.")
#             self._warned_about_missing_keys = True

#         return

#     def on_batch(
#         self,
#         trainer: "GradientDescentTrainer",
#         batch_inputs: List[List[TensorDict]],
#         batch_outputs: List[Dict[str, Any]],
#         batch_metrics: Dict[str, Any],
#         epoch: int,
#         batch_number: int,
#         is_training: bool,
#         is_primary: bool = True,
#         batch_grad_norm: Optional[float] = None,
#         **kwargs: Any,
#     ) -> None:
#         # do everything as the parent does
#         super().on_batch(
#             trainer,
#             batch_inputs,
#             batch_outputs,
#             batch_metrics,
#             epoch,
#             batch_number,
#             is_training,
#             is_primary=is_primary,
#             batch_grad_norm=batch_grad_norm,
#             **kwargs,
#         )
#         assert len(batch_outputs) == 1, "Gradient accumulation not supported"
#         self._warn_about_missing_keys(batch_outputs)

#         for key in self._model_outputs_to_log:
#             value = batch_outputs[0].get(key, None)

#             if value is not None:
#                 if is_training:
#                     self._tensorboard.add_train_histogram(  # type: ignore
#                         "model_outputs/" + key, value
#                     )
# # @TrainerCallback.register("track_epoch_callback")
# # class TrackEpochCallback(TrainerCallback):
# #     """
# #     A callback that you can pass to the `GradientDescentTrainer` to access the current epoch number
# #     in your model during training. This callback sets `model.epoch`, which can be read inside of
# #     `model.forward()`. We set `model.epoch = epoch + 1` which now denotes the number of
# #     completed epochs at a given training state.
# #     """
# #     def on_start(
# #         self, trainer: "GradientDescentTrainer", is_primary: bool = True, **kwargs
# #     ) -> None:
# #         super().on_start(trainer, is_primary)
# #         trainer.model.apply(lambda module: module.epoch = 0)
# #         trainer.model.epoch = 0
# #     def on_epoch(
# #         self,
# #         trainer: "GradientDescentTrainer",
# #         metrics: Dict[str, Any],
# #         epoch: int,
# #         is_primary: bool = True,
# #         **kwargs,
# #     ) -> None:
# #         trainer.model.apply(lambda module: module.epoch = module.epoch+1)
# #         trainer.model.epoch = epoch + 1